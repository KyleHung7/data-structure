import os
import json
import pandas as pd
import gradio as gr
from dotenv import load_dotenv
import google.generativeai as genai

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
gemini_api_key = os.getenv("GEMINI_API_KEY")

if not gemini_api_key:
    raise ValueError("âŒ ç’°å¢ƒè®Šæ•¸ GEMINI_API_KEY æœªè¨­å®šï¼è«‹ç¢ºèª .env æª”æ¡ˆå…§å®¹")

# è¨­å®š Gemini API
genai.configure(api_key=gemini_api_key)
model = genai.GenerativeModel("gemini-1.5-flash")

# è©•ä¼°é …ç›®
ITEMS = ["ç©æ¥µé¢å‘ç¸½çµ", "äº®é»æŒ‡å‡º", "ä¿¡å¿ƒæå‡å»ºè­°", "æ¿€å‹µè³‡è¨Šè£œå……"]

def format_list_as_numbered_string(items):
    """å°‡åˆ—è¡¨è½‰æ›ç‚ºæ¢åˆ—å¼å­—ä¸²"""
    return "\n".join([f"{i+1}. {item}" for i, item in enumerate(items) if item.strip()])

def parse_response(response_text):
    """è§£æ Gemini API å›æ‡‰ JSONï¼Œç¢ºä¿æ­£ç¢ºæ ¼å¼"""
    cleaned = response_text.strip()
    
    if cleaned.startswith("```"):
        lines = cleaned.splitlines()
        lines = lines[1:] if lines[0].startswith("```") else lines
        lines = lines[:-1] if lines[-1].strip() == "```" else lines
        cleaned = "\n".join(lines).strip()
    
    try:
        result = json.loads(cleaned)
        return {item: format_list_as_numbered_string(result.get(item, [])) for item in ITEMS}
    except json.JSONDecodeError:
        return {item: "" for item in ITEMS}


def process_batch_dialogue(grouped_dialogues):
    """æ‰¹é‡è™•ç†æ—¥èªŒï¼Œæ¯å¤©çš„å…§å®¹ç¨ç«‹ç™¼é€è«‹æ±‚"""
    results = []
    
    for dialogue in grouped_dialogues:
        prompt = (
            "ä½ æ˜¯ä¸€ä½èªè¨€åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è©•ä¼°é …ç›®åˆ†ææ—¥èªŒå…§å®¹ï¼š\n"
            + "\n".join(ITEMS) +
            "\n\nè«‹ç”¢ç”Ÿ JSON æ ¼å¼å›æ‡‰ï¼Œæ¯å€‹é …ç›®éœ€æä¾› 3 å€‹å…·é«”è©•ä¼°è¨Šæ¯ã€‚\n"
            "ç¯„ä¾‹è¼¸å‡ºï¼š\n```json\n{\n  \"ç©æ¥µé¢å‘ç¸½çµ\": [\"å…§å®¹1\", \"å…§å®¹2\", \"å…§å®¹3\"],\n  \"äº®é»æŒ‡å‡º\": [\"å…§å®¹1\", \"å…§å®¹2\", \"å…§å®¹3\"],\n  ...\n}\n```"
            f"\n\næ—¥èªŒå…§å®¹ï¼š{dialogue}"
        )

        response = model.generate_content(prompt)

        # è§£æå›æ‡‰
        parsed_result = parse_response(response.text)
        results.append(parsed_result)

    return results

def process_file(file_obj, chat_history):
    """è™•ç†ä¸Šå‚³çš„ CSV ä¸¦é€²è¡Œåˆ†æ"""
    if not file_obj:
        return chat_history, None

    try:
        # è®€å– CSVï¼Œé¿å…å›  BOM è€Œå°è‡´æ¬„ä½åç¨±éŒ¯èª¤
        df = pd.read_csv(file_obj.name, encoding="utf-8-sig")

        # æ¨™æº–åŒ–æ¬„ä½åç¨±ï¼Œå»é™¤å¤šé¤˜ç©ºç™½
        df.columns = df.columns.str.strip()

        # ç¢ºä¿ CSV æ¬„ä½åç¨±æ­£ç¢º
        expected_columns = ["æ—¥èªŒæ—¥æœŸ", "æ—¥èªŒå…§å®¹"]
        missing_columns = [col for col in expected_columns if col not in df.columns]

        if missing_columns:
            raise ValueError(f"âš ï¸ CSV æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{', '.join(missing_columns)}")

        # è½‰æ›æ—¥æœŸæ ¼å¼ï¼Œç¢ºä¿åˆ†çµ„ä¸æœƒéŒ¯èª¤
        df["æ—¥èªŒæ—¥æœŸ"] = pd.to_datetime(df["æ—¥èªŒæ—¥æœŸ"], errors="coerce").dt.strftime("%Y-%m-%d")

        # ç¢ºä¿æ²’æœ‰ NaN æ—¥æœŸ
        df = df.dropna(subset=["æ—¥èªŒæ—¥æœŸ"])

        # ç¢ºä¿æ—¥èªŒå…§å®¹æ˜¯å­—ä¸²æ ¼å¼
        df["æ—¥èªŒå…§å®¹"] = df["æ—¥èªŒå…§å®¹"].astype(str)

        # ä»¥æ—¥æœŸç‚ºå–®ä½åˆ†çµ„
        grouped = df.groupby("æ—¥èªŒæ—¥æœŸ", as_index=False).agg({"æ—¥èªŒå…§å®¹": "\n".join})

        chat_history.append({"role": "system", "content": f"ğŸ“„ æ­£åœ¨åˆ†æ {len(grouped)} å¤©çš„æ—¥èªŒ..."})

        batch_size = 5
        total = len(grouped)
        output_data = []

        for start_idx in range(0, total, batch_size):
            end_idx = min(start_idx + batch_size, total)
            dialogues = grouped.iloc[start_idx:end_idx]["æ—¥èªŒå…§å®¹"].tolist()
            
            # ç¢ºä¿æ¯å€‹æ—¥èªŒå…§å®¹åˆ†é–‹è™•ç†
            batch_results = process_batch_dialogue(dialogues)

            for idx, row in enumerate(grouped.iloc[start_idx:end_idx].itertuples(index=False)):
                result = batch_results[idx]  # ç¢ºä¿ç´¢å¼•å°æ‡‰æ­£ç¢º

                if all(not result[item] for item in ITEMS):
                    continue

                output_data.append({
                    "æ—¥èªŒæ—¥æœŸ": row.æ—¥èªŒæ—¥æœŸ,  # ç¢ºä¿æ—¥æœŸä¸æœƒåˆä½µ
                    "æ—¥èªŒå…§å®¹": row.æ—¥èªŒå…§å®¹,
                    **result
                })

                # ç¨ç«‹åŠ å…¥ chat_historyï¼Œè€Œä¸æ˜¯åˆä½µ
                chat_history.append({
                    "role": "assistant",
                    "content": f"ğŸ“… **{row.æ—¥èªŒæ—¥æœŸ}**\n\n" + 
                               "\n\n".join([f"**{k}**:\n{v}" for k, v in result.items() if v])
                })

        if not output_data:
            chat_history.append({"role": "system", "content": "âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„åˆ†æçµæœï¼Œè«‹ç¢ºèªæ—¥èªŒå…§å®¹æ ¼å¼ã€‚"})
            return chat_history, None

        output_df = pd.DataFrame(output_data)
        output_file = "journalUI_output.csv"
        output_df.to_csv(output_file, index=False, encoding="utf-8-sig")

        chat_history.append({"role": "system", "content": "âœ… åˆ†æå®Œæˆï¼ä¸‹è¼‰çµæœï¼š"})
        return chat_history, output_file

    except Exception as e:
        chat_history.append({"role": "system", "content": f"âŒ è§£æå¤±æ•—ï¼š{str(e)}"})
        return chat_history, None

# å»ºç«‹ Gradio ä»‹é¢
with gr.Blocks() as demo:
    gr.Markdown("### ğŸ“– AI æ—¥èªŒåˆ†æç³»çµ±")

    file_input = gr.File(label="ä¸Šå‚³æ—¥èªŒ CSV")
    chat_display = gr.Chatbot(label="åˆ†æéç¨‹", type="messages")
    download_log = gr.File(label="ä¸‹è¼‰åˆ†æçµæœ")

    start_btn = gr.Button("é–‹å§‹åˆ†æ")

    start_btn.click(
        fn=process_file,
        inputs=[file_input, chat_display],
        outputs=[chat_display, download_log]
    )

# å•Ÿå‹• Web ä»‹é¢
demo.queue().launch()
